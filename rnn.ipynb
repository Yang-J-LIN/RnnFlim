{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Zp7CQGpT3Tzq"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check available GPUs\n","\n","if torch.cuda.is_available() is True:\n","    for i in range(torch.cuda.device_count()):\n","        print('Device {}:'.format(i), torch.cuda.get_device_name(i))\n","\n","device = 'cuda:0'   # use Nvidia RTX A4500 GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9u88hK-N4STc"},"outputs":[],"source":["n_bit = 12\n","# scaler = 2 ** 12\n","scaler = 1000"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zisnybG43Tzt"},"source":["# 1. Define and instantiate simulation dataset\n","\n","The file to be loaded should be saved with '.npy' format. And the data and labels are supposed to be saved seperately."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4PWI_fYE3Tzu"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class SimulationDataset(Dataset):\n","    def __init__(self, data_path, label_path, data_raw_path='', noises_path='', device='cuda') -> None:\n","        super().__init__()\n","        self.data_path = data_path\n","        self.label_path = label_path\n","        self.data_raw_path = data_raw_path\n","        self.noises_path = noises_path\n","\n","        self.device = device\n","\n","        self.data = np.load(self.data_path).astype(np.float32)\n","        self.labels = np.load(self.label_path).astype(np.float32)\n","\n","        self.data = torch.tensor(self.data)\n","        self.labels = torch.tensor(self.labels)\n","\n","        if noises_path != '':\n","            self.noises = np.load(noises_path)\n","            # self.noises = torch.tensor(self.noises) / (2 ** n_bit)\n","            self.noises = torch.tensor(self.noises) / scaler\n","\n","        # self.data = torch.tensor(self.data, device=device)\n","        # self.labels = torch.tensor(self.labels, device=device)\n","\n","\n","        # self.data_max = torch.max(self.data, dim=1).values\n","        # print(self.data_max.shape)\n","        # self.data = self.data / self.data_max.unsqueeze(1)\n","\n","        # self.data = self.data / (2 ** n_bit)\n","        self.data = self.data / scaler\n","        self.labels = self.labels\n","    def __len__(self):\n","        assert len(self.data) == len(self.labels)\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        # sorted_data, _ = torch.sort(self.data[index])\n","        # return sorted_data, self.labels[index][-2]\n","        # return self.data[index].to(self.device), self.labels[index][-2].to(self.device)\n","        num_timestamps = 1024\n","        if self.noises_path != '':\n","            return self.data[index, 0:num_timestamps].to(self.device), self.labels[index].to(self.device), self.noises[index, 0:num_timestamps].to(self.device)\n","        else:\n","            return self.data[index, 0:num_timestamps].to(self.device), self.labels[index].to(self.device)\n","        # return self.data[index], self.labels[index][-2]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AnQ4aKfU3Tzv"},"source":["Install the googledrivedonwloader and download [datasets](https://drive.google.com/drive/folders/1FdoF63YvHkfgpiSFmD5dKMZdPIP13ojs?usp=sharing)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hon7lYT3Tzx"},"outputs":[],"source":["dataset_path = 'dataset'\n","\n","simulation_dataset = SimulationDataset(\n","    data_path = dataset_path + 'data.npy',\n","    label_path = dataset_path + 'label.npy',\n","    device = device\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"elapsed":4523,"status":"error","timestamp":1661778291513,"user":{"displayName":"Yang Lin","userId":"17921802293034261563"},"user_tz":-120},"id":"FvhyP7S03Tzy","outputId":"d4cfe134-de2e-40e6-f9cd-76479c57ecb0"},"outputs":[],"source":["data_sample = simulation_dataset[np.random.randint(0, len(simulation_dataset))]\n","print(data_sample[0])\n","print(data_sample[0].shape, data_sample[1].shape)\n","sns.histplot(data_sample[0].detach().cpu().numpy())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GGsPA-bk3Tzy"},"source":["## 2.3 Split dataset and define dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1661945443203,"user":{"displayName":"Yang Lin","userId":"17921802293034261563"},"user_tz":-120},"id":"FhFMybD_3Tzz","outputId":"12be049a-e20e-409f-e914-ee1f4a76a482"},"outputs":[],"source":["from torch.utils.data import random_split\n","train_set_size = int(0.8 * len(simulation_dataset))\n","dev_set_size = int(0.1 * len(simulation_dataset))\n","test_set_size = len(simulation_dataset) - train_set_size - dev_set_size\n","print('train/dev/test size is {}/{}/{}.'.format(\n","    train_set_size,\n","    dev_set_size,\n","    test_set_size\n","))\n","train_set, dev_set, test_set = \\\n","    random_split(simulation_dataset, [train_set_size, dev_set_size, test_set_size], generator=torch.Generator().manual_seed(42))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQccWr8T3Tzz"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 128\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size)\n","dev_loader = DataLoader(dev_set, batch_size=batch_size)\n","test_loader = DataLoader(test_set, batch_size=batch_size)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"24UmBRCo3Tzz"},"source":["# 2. Define model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNNQuCUq3Tz0"},"outputs":[],"source":["import torch.nn as nn\n","from torch.nn import GRU, LSTM, RNN\n","\n","class RNNModel(nn.Module):\n","    def __init__(self, rnn='gru', hidden_size=32, num_layers=1, keep_state=True):\n","        super().__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.keep_state = keep_state\n","\n","        self.rnn_type = rnn\n","\n","        # print(self.rnn_type)\n","\n","        if rnn == 'gru':\n","            self.rnn = GRU(\n","                input_size=1,\n","                hidden_size=hidden_size,\n","                num_layers=num_layers,\n","                batch_first=True\n","            )\n","        elif rnn == 'rnn':\n","            self.rnn = RNN(\n","                input_size=1,\n","                hidden_size=hidden_size,\n","                num_layers=num_layers,\n","                batch_first=True\n","            )\n","        else:\n","            self.rnn = LSTM(\n","                input_size=1,\n","                hidden_size=hidden_size,\n","                num_layers=num_layers,\n","                batch_first=True\n","            )\n","    \n","        self.linear1 = nn.Linear(hidden_size, hidden_size//2)\n","        self.linear2 = nn.Linear(hidden_size//2, 1)\n","\n","\n","    def forward(self, x, hn=None):\n","        if self.keep_state is False or hn is None:\n","            output, hn = self.rnn(x)\n","        else:\n","            output, hn = self.rnn(x, hn)\n","\n","        output = torch.relu(self.linear1(output))\n","        output = self.linear2(output)\n","        output = output.squeeze(2)\n","\n","        # print(self.rnn)\n","        if (self.rnn_type == 'rnn') or (self.rnn_type == 'gru'):\n","            return output, hn.detach()\n","        elif self.rnn_type == 'lstm':\n","            return output, (hn[0].detach(), hn[1].detach())\n","\n","\n","    def _reinitialize(self):\n","        \"\"\"\n","        Tensorflow/Keras-like initialization\n","        \"\"\"\n","        for name, p in self.named_parameters():\n","            if 'rnn' in name:\n","                if 'weight_ih' in name:\n","                    nn.init.xavier_uniform_(p.data)\n","                elif 'weight_hh' in name:\n","                    nn.init.orthogonal_(p.data)\n","                elif 'bias_ih' in name:\n","                    p.data.fill_(0)\n","                    # Set forget-gate bias to 1\n","                    n = p.size(0)\n","                    p.data[(n // 4):(n // 2)].fill_(1)\n","                elif 'bias_hh' in name:\n","                    p.data.fill_(0)\n","            elif 'fc' in name:\n","                if 'weight' in name:\n","                    nn.init.xavier_uniform_(p.data)\n","                elif 'bias' in name:\n","                    p.data.fill_(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaYreOZz3Tz0"},"outputs":[],"source":["model = RNNModel(rnn='gru', hidden_size=12, num_layers=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpVIEKoJ3Tz0"},"outputs":[],"source":["model._reinitialize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ca279Kjv3Tz1"},"outputs":[],"source":["def get_n_params(model):\n","    pp=0\n","    for p in list(model.parameters()):\n","        nn=1\n","        for s in list(p.size()):\n","            nn = nn*s\n","        pp += nn\n","    return pp\n","\n","\n","n_paras = get_n_params(model.rnn)\n","print('Parameter number of model:', n_paras)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"07o17R713Tz1"},"source":["# 3. Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5242,"status":"ok","timestamp":1661945521151,"user":{"displayName":"Yang Lin","userId":"17921802293034261563"},"user_tz":-120},"id":"brZZ6x4wDfJd","outputId":"70aeebdf-4db9-4d02-bb2d-80ec4ac37e3b"},"outputs":[],"source":["if device == 'cpu':\n","    pass\n","else:\n","    model.to(device)\n","    print(\"Training with CUDA.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jBqALek3Tz1"},"outputs":[],"source":["import torch.optim as optim\n","\n","learning_rate = 0.001\n","\n","optimizer = optim.AdamW(lr=learning_rate, params=model.parameters())\n","\n","scheduler = optim.lr_scheduler.StepLR(\n","    optimizer=optimizer,\n","    step_size=5,\n","    gamma=0.9\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8BT4lFC3Tz1"},"outputs":[],"source":["import torch.nn as nn\n","\n","weights = torch.sigmoid(torch.tensor((np.arange(1024) - 256) / 256)).reshape(1, -1).to(device)\n","\n","\n","def loss_func(pred, label):\n","  \n","    # print(pred.shape, label.shape)\n","    label = label.repeat(1024, 1).T\n","    # print(label.shape)\n","    error = (pred - label) ** 2 / (label ** 2)\n","    error = error * weights\n","    error = torch.mean(error)\n","    return error\n","\n","\n","criterion = loss_func"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datetime import datetime\n","\n","now = datetime.now()\n","date = now.strftime(\"%Y%m%d_%H%M%S\")\n","\n","model_type = 'gru'\n","\n","model_para = str(model.rnn.hidden_size)\n","filename = '_'.join([date, model_type, model_para])\n","\n","print(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2265044,"status":"error","timestamp":1661953114818,"user":{"displayName":"Yang Lin","userId":"17921802293034261563"},"user_tz":-120},"id":"XYxrc6Kr3Tz2","outputId":"9c833bc6-7bdd-4ff0-a0f5-293ef1113dcf"},"outputs":[],"source":["from tqdm import tqdm\n","\n","epoch = 100\n","\n","hn = None\n","\n","for i in range(epoch):\n","    loss_total = 0\n","    for j, (data, label) in tqdm(enumerate(train_loader)):\n","\n","        data = data.unsqueeze(2)\n","\n","        output, hn = model(data, None)\n","\n","        loss = criterion(output, label)\n","\n","        loss_total += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    torch.save(model.state_dict(), os.path.join('checkpoints', filename + '.pt'))\n","\n","    print('Finished training epoch {}. Average loss is {:.8f}'.format(i + 1, loss_total / len(train_loader)))\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","record_path = 'checkpoints/record.csv'\n","\n","record = {\n","    'filename': [filename],\n","    'dataset_path': [dataset_path]\n","}\n","\n","record = pd.DataFrame(record)\n","\n","if os.path.exists(record_path):\n","    df = pd.read_csv(record_path)\n","    df = pd.concat([df, record], ignore_index=True)\n","    df.to_csv(record_path, index=False)\n","else:\n","    record.to_csv(record_path, index=False)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4974,"status":"ok","timestamp":1661953127282,"user":{"displayName":"Yang Lin","userId":"17921802293034261563"},"user_tz":-120},"id":"9Ch5jrCT3Tz2","outputId":"b4239d70-d000-4888-d77f-b5d079410ac7"},"outputs":[],"source":["results = []\n","labels = []\n","for data, label in dev_loader:\n","# for data, label, _ in dev_loader:\n","# for data, label, _, _ in dev_loader:\n","\n","    data = data.unsqueeze(2)\n","    # output = model(data)[:, -1]\n","    output, _ = model(data)\n","    # output = torch.mean(model(data), dim=1)\n","\n","    results.append(output.detach().cpu().numpy())\n","    labels.append(label.detach().cpu().numpy())\n","\n","    # break\n","\n","\n","results = np.concatenate(results, axis=0)\n","labels = np.concatenate(labels, axis=0)\n","print(results.shape)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456},"executionInfo":{"elapsed":2021,"status":"ok","timestamp":1661953263270,"user":{"displayName":"Yang Lin","userId":"17921802293034261563"},"user_tz":-120},"id":"gR7nEQm53Tz3","outputId":"30c51bdf-35f2-46b3-c69d-25ff48c353cd"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","\n","# results_to_plot = [\n","#     results[:, -1],\n","#     labels,\n","#     results[:, -1].squeeze() - labels,\n","#     np.abs(results[:, -1].squeeze() - labels) / labels\n","# ]\n","\n","idx = 1023\n","\n","results_to_plot = [\n","    results[:, idx],\n","    labels,\n","    results[:, idx].squeeze() - labels,\n","    np.abs(results[:, idx].squeeze() - labels) / labels\n","]\n","\n","print('Average absolute error is:', np.mean(np.abs(results_to_plot[2])))\n","print('Root mean squared error is:', np.sqrt(mean_squared_error(labels, results[:, -1])))\n","print('Mean absolute percentage error is:', np.mean(np.abs((results[:, idx] - labels) / labels)))\n","\n","titles = [\n","    'results',\n","    'labels',\n","    'errors',\n","    'errors %'\n","]\n","\n","fig, ax = plt.subplots(ncols=4)\n","fig.set_size_inches((20, 6))\n","for col, to_plot, title in zip(ax, results_to_plot, titles):\n","    sns.histplot(to_plot, ax=col)\n","    col.set_title(title)\n","    col.set_xlabel('(ns)')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405},"executionInfo":{"elapsed":1600,"status":"ok","timestamp":1661953280218,"user":{"displayName":"Yang Lin","userId":"17921802293034261563"},"user_tz":-120},"id":"LZhFNdCd3Tz3","outputId":"83e30dc5-3877-4169-eb3c-6228ac5290e2"},"outputs":[],"source":["df = pd.DataFrame({'pred': results[:, -1], 'truth': labels})\n","\n","plt.figure(figsize=(6, 6))\n","sns.scatterplot(x=results[:, -1], y=labels, alpha=0.05)\n","plt.plot([0.2, 5], [0.2, 5], '-r')\n","plt.xlabel('Prediction (ns)')\n","plt.ylabel('Ground Truth (ns)')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AW0oxTc4ItLB"},"source":["## Save the weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.cat([model.rnn.bias_ih_l0[0:2*model.hidden_size] + model.rnn.bias_hh_l0[0:2*model.hidden_size], model.rnn.bias_ih_l0[2*model.hidden_size:], model.rnn.bias_hh_l0[2*model.hidden_size:]]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","def save_weights(model, path, lstm_size=12):\n","    if not os.path.exists(path):\n","        os.mkdir(path)\n","    Wih = model.rnn.weight_ih_l0.T.detach().cpu().numpy().reshape(1, -1)\n","    Whh = model.rnn.weight_hh_l0.T.detach().cpu().numpy().reshape(1, -1)\n","\n","    B = torch.cat([\n","        model.rnn.bias_ih_l0[0:2*model.hidden_size] + model.rnn.bias_hh_l0[0:2*model.hidden_size],\n","        model.rnn.bias_ih_l0[2*model.hidden_size:],\n","        model.rnn.bias_hh_l0[2*model.hidden_size:]]).T.detach().cpu().numpy().reshape(1, -1)\n","\n","    np.savetxt(os.path.join(path, \"rnn.weight_ih_l0.txt\"), Wih, delimiter=\",\")\n","    np.savetxt(os.path.join(path, \"rnn.weight_hh_l0.txt\"), Whh, delimiter=\",\")\n","    np.savetxt(os.path.join(path, \"rnn.bias.txt\"), B, delimiter=\",\")\n","    np.savetxt(os.path.join(path, \"linear1.weight.txt\"), model.linear1.weight.T.detach().cpu().numpy().reshape(1, -1), delimiter=\",\")\n","    np.savetxt(os.path.join(path, \"linear1.bias.txt\"), model.linear1.bias.T.detach().cpu().numpy().reshape(1, -1), delimiter=\",\")\n","    np.savetxt(os.path.join(path, \"linear2.weight.txt\"), model.linear2.weight.T.detach().cpu().numpy().reshape(1, -1), delimiter=\",\")\n","    np.savetxt(os.path.join(path, \"linear2.bias.txt\"), model.linear2.bias.T.detach().cpu().numpy().reshape(1, -1), delimiter=\",\")\n","\n","save_weights(model, 'weights')"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["2dJj7p8BBrEh","-kc_yfEOSeNY","TZknDTrsYVy8","6tTjxW3K-e3_"],"machine_shape":"hm","name":"rnn.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"2f9c97a4ed2c1e1ce8a4db3596bfba351bf37e0589944a32d9295b520b352f82"}}},"nbformat":4,"nbformat_minor":0}
